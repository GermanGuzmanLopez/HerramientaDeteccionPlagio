As one of the branches of machine learning, the deep learning model combined with artificial intelligence is widely used in the field of computer vision technology, and the image recognition field represented by medical image analysis is also developing. Its strength lies in the ability to function without human intervention, allowing the system to detect and analyze features that may be missed by human analysts, thereby achieving or surpassing human-level accuracy. Based on the general lack of explainability caused by the unknown data processing process in the deep model, the existing solutions mainly include the establishment of internal explainability, attention mechanism interpretation of specific models, and the interpretation of unknowable models represented by LIME. Quantitative methods for evaluating interpretability are still under investigation, particularly in terms of the evaluative frameworks used by medical professionals and patients in decision-making contexts, with several scales being developed for reference. The current research on the application of artificial intelligence deep learning models in medical imaging generally pays more attention to accuracy rather than explainability, resulting in the lack of explainability, and thus hindering the practical clinical application of deep learning models. This focus on precision over interpretability creates a barrier to the adoption of these models in real-world clinical settings. Therefore, the need to analyze the development of medical image analysis in the field of artificial intelligence and computer vision technology, and how to balance accuracy and interpretability to develop deep learning models that both doctors and patients can trust will become the research focus of the industry in the future. Addressing this balance will be crucial for integrating AI-driven models into everyday medical practice.