This paper presents a method for providing explainability in the integration of artificial intelligence (AI) and data mining techniques when dealing with meteorological prediction. This study introduces a method to enhance explainability in the fusion of AI and data mining techniques for meteorological forecasting. Explainable artificial intelligence (XAI) refers to the transparency of AI systems in providing explanations for their predictions and decision-making processes, and contribute to improve prediction accuracy and enhance trust in AI systems. Explainable AI (XAI) aims to ensure AI systems can transparently provide rationales for their predictions and decisions, thereby improving accuracy and fostering trust. The focus of this paper relies on the interpretability challenges in ordinal classification problems within weather forecasting. This paper primarily addresses the interpretability issues in ordinal classification problems related to weather prediction. Ordinal classification involves predicting weather phenomena with ordered classes, such as temperature ranges, wind speed, precipitation levels, and others. Ordinal classification entails forecasting weather phenomena categorized into ordered classes, such as temperature bands, wind speeds, precipitation levels, etc. To address this challenge, a novel and general explicable forecasting framework, that combines inductive rules and fuzzy logic, is proposed in this work. To tackle this issue, we propose a novel general explainable forecasting framework that integrates inductive rules and fuzzy logic. Inductive rules, derived from historical weather data, provide a logical and interpretable basis for forecasting; while fuzzy logic handles the uncertainty and imprecision in the weather data. Inductive rules generated from historical weather data offer a logical and interpretable foundation for forecasting, while fuzzy logic addresses the uncertainty and imprecision inherent in weather data. The system predicts a set of probabilities that the incoming sample belongs to each considered class. The system outputs probabilities indicating the likelihood of the incoming sample belonging to each class. Moreover, it allows the expert decision-making process to be strengthened by relying on the transparency and physical explainability of the model, and not only on the output of a black-box algorithm. Furthermore, it bolsters expert decision-making by providing transparency and physical explainability, rather than relying solely on the outputs of a black-box algorithm. The proposed framework is evaluated using two real-world weather databases related to wind speed and low-visibility events due to fog. We evaluate the proposed framework with two real-world weather databases focusing on wind speed and low-visibility events caused by fog. The results are compared to both ML classifiers and specific methods for ordinal classification problems, achieving very competitive results in terms of ordinal performance metrics while offering a higher level of explainability and transparency compared to existing approaches. The results, compared to traditional ML classifiers and specific ordinal classification methods, show highly competitive performance metrics, while also providing superior explainability and transparency over current methods.