Interpretable artificial intelligence (AI), also known as explainable AI, becomes essential in the establishment of trustworthy AI for translation from bench-to-bedside, carrying substantial implications for human well-being. However, the focus of much research in this field has been on the design of complex and sophisticated methods, disregarding their interpretability. As a result, the primary requirement for the implementation of reliable AI in medical domains remains unfulfilled. Various explanation methods for interpretable AI have been devised by scientists. Among these methods, fuzzy rules have emerged as a novel and powerful tool embedded in a fuzzy inference system (FIS), bridging the communication gap between humans and advanced AI machines. However, there has been limited assessment of the use of FISs in medical diagnosis. Moreover, insufficient attention has been given to the application of fuzzy rules to different types of multimodal medical data, despite their potential in designing appropriate methodologies for available datasets. This review aims to provide a foundational understanding of interpretability and fuzzy rules, conducting comparative analyses of the utilization of fuzzy rules and other explanation methods in managing three major types of multimodal data (i.e., sequence signals, medical images, and tabular data), and presenting insights into suitable scenarios for fuzzy rule application and recommendations for future research.