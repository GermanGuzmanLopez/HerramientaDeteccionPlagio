The establishment of trustable AI for bench-to-bedside translation, with significant implications for human well-being, relies heavily on interpretable artificial intelligence (AI), also known as explainable AI. Despite this necessity, most current research has focused on developing intricate and sophisticated methods, often overlooking their interpretability. As a result, the primary requirement for deploying trustworthy AI in medical fields has not been satisfied. Various explanation methods for interpretable AI have been developed by scientists. Among these, fuzzy rules within a fuzzy inference system (FIS) have proven to be a novel and effective tool to bridge the communication gap between humans and advanced AI systems. However, there is a lack of reviews on the use of FISs in medical diagnosis. Additionally, the application of fuzzy rules to different types of multimodal medical data has not received enough attention, even though fuzzy rules have potential in designing suitable methodologies for the available datasets. This review aims to provide a basic understanding of interpretability and fuzzy rules, compare the use of fuzzy rules and other explanation methods in managing three main types of multimodal data (i.e., sequence signals, medical images, and tabular data), and offer insights into suitable scenarios for applying fuzzy rules along with recommendations for future research.