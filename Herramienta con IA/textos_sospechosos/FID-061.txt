Deep learning models, a subset of machine learning combined with artificial intelligence, are extensively utilized in the domain of computer vision technology, with significant advancements in the field of image recognition, particularly in medical image analysis. One key benefit of these models is their ability to function without human annotation, allowing computers to identify and process feature information that may be overlooked during human-driven model training, thereby achieving or even surpassing human accuracy. Given the general lack of explainability due to the opaque data processing in deep models, current solutions primarily involve developing internal explainability, using attention mechanisms to interpret specific models, and utilizing LIME to explain otherwise opaque models. The quantitative assessment of interpretability is still a developing field, particularly in evaluating medical decision-related models from the perspectives of both doctors and patients, with several proposed scales for reference. Present research on applying artificial intelligence deep learning models to medical imaging tends to prioritize accuracy over explainability, leading to a deficit in explainability that impedes the practical clinical implementation of these models. Hence, analyzing the evolution of medical image analysis within artificial intelligence and computer vision technology, and finding a balance between accuracy and interpretability to develop deep learning models that are trusted by both doctors and patients, will become a key research focus in the industry moving forward.