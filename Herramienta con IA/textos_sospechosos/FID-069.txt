Explainable artificial intelligence (AI), also referred to as interpretable AI, is crucial for developing trustworthy AI systems for bench-to-bedside applications, significantly impacting human well-being. Nevertheless, most research in this field has focused on creating complex and sophisticated methods without considering their interpretability. As a result, the fundamental requirement for implementing reliable AI in medical fields has not been achieved. Various explanation methods for interpretable AI have been developed by scientists. Among these, fuzzy rules integrated within a fuzzy inference system (FIS) have emerged as a novel and effective tool to close the communication gap between humans and advanced AI systems. However, there have been limited reviews on the use of FISs in medical diagnosis. Furthermore, the application of fuzzy rules to different types of multimodal medical data has not received adequate attention, despite their potential for designing appropriate methodologies for the available datasets. This review aims to provide a foundational understanding of interpretability and fuzzy rules, conduct comparative analyses of fuzzy rules and other explanation methods for managing three major types of multimodal data (i.e., sequence signals, medical images, and tabular data), and offer insights into suitable scenarios for applying fuzzy rules along with recommendations for future research.