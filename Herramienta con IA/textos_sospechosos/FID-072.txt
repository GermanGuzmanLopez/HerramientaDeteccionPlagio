In the rapidly evolving field of human-computer interaction, this paper presents a groundbreaking framework set to transform chatbot systems. The framework is carefully designed for emotionally intelligent multimodal chatbots, integrating real-time facial and emotion recognition with natural language processing. Although still in the developmental phase, the framework demonstrates a unique capability in executing a wide range of human instructions. These tasks include generating detailed captions, counting objects, and responding to general queries. By utilizing parameter-efficient fine-tuning from OpenFlamingo, enhanced with the Low-rank Adapter (LoRA), our framework emphasizes versatility and operational efficiency. At the core of our approach is the creation of instruction templates that intricately combine both vision and language data. This strategic integration forms the basis of our multi-modality instruction tuning methodology, significantly improving the chatbot's adaptability. We highlight the crucial role of high-quality training data in shaping the chatbot's dialogue performance. As the framework continues to develop, this research establishes a strong foundation for creating a versatile conversational agent. The potential applications of this agent range from improving customer service experiences to offering valuable support in mental health scenarios.