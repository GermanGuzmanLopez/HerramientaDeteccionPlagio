Facial Expression Recognition (FER) finds application in diverse fields including education, gaming, robotics, healthcare, and more. Through techniques such as interactive robots integrated with Artificial Intelligence, FER enables the recognition of human faces and the detection of emotions during conversations, allowing for tailored responses based on the user's emotional state. One intriguing application of FER is in playing music based on the user's mood, achieved by analyzing facial expressions to infer their feelings.

However, existing emotion models often struggle to accurately capture the connection between music and facial emotions, warranting further investigation. In this paper, we propose a Convolutional Neural Network (CNN) based deep learning approach to address this challenge. Deep learning, known for its ability to effectively analyze unstructured data, movies, and various forms of media, offers a promising solution.

Our research culminates in the development of a real-time system capable of recognizing human faces, assessing emotions, and recommending music to users based on their facial expressions. Leveraging the OAHEGA and FER-2013 datasets for experimental validation, we create and train two emotion recognition models using various combinations of these datasets, achieving an accuracy of 73.02%. With our CNN model, we can predict six emotions: anger, fear, joy, neutral, sadness, and surprise.

The proposed system holds potential for deployment in various settings where real-time facial recognition is pivotal. Its applications extend to contexts where personalized interactions based on emotional cues can enhance user experiences. Overall, our research contributes to the advancement of FER technology and its practical implementation in real-world scenarios.