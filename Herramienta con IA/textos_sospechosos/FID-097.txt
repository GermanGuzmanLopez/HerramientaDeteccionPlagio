Facial Expression Recognition (FER) finds applications across diverse fields such as education, gaming, robotics, healthcare, and more. For instance, interactive robots integrated with Artificial Intelligence leverage facial expression techniques to discern human emotions during conversations, enabling them to select appropriate responses. One compelling use case involves playing music tailored to the user's mood, wherein facial emotion detection informs the selection of music genres or tracks based on the user's emotional state.

However, the efficacy of existing emotion models in capturing the relationship between music and facial expressions warrants further investigation. In this paper, we employ a Convolutional Neural Network (CNN) based deep learning approach to address this challenge. Deep learning techniques excel at analyzing unstructured data, including movies and media, surpassing the capabilities of traditional machine learning methods.

Our research culminates in the development of a real-time system capable of recognizing human faces, assessing emotions, and recommending music to users based on their facial expressions. To evaluate our approach, we utilize the OAHEGA and FER-2013 datasets for experimental validation. We create and train two emotion recognition models using various combinations of these datasets, achieving an accuracy of 73.02%.

With our CNN-based model, we can predict six emotions: anger, fear, joy, neutral, sadness, and surprise. The proposed system holds promise for deployment across diverse settings where real-time facial recognition is integral. Its applications extend to contexts where personalized interactions and responses based on emotional cues are paramount, making it a valuable tool in enhancing user experiences.