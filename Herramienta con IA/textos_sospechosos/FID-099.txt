Interpretable artificial intelligence (AI), also referred to as explainable AI, plays a crucial role in fostering trust in AI systems, especially in applications like bench-to-bedside translation in the medical domain, where human well-being is at stake. However, much of the existing research in this field has focused on developing complex and sophisticated AI methods, often neglecting their interpretability. As a result, the essential requirement for implementing trustworthy AI in medical contexts remains unmet.

To address this gap, scientists have explored various explanation methods for interpretable AI, with fuzzy rules embedded in fuzzy inference systems (FISs) emerging as a promising tool for bridging the communication divide between humans and advanced AI systems. Despite their potential, there has been limited review of the application of FISs in medical diagnosis. Additionally, the utilization of fuzzy rules across different types of multimodal medical data has received inadequate attention, despite the potential for designing suitable methodologies for available datasets.

This review aims to provide a foundational understanding of interpretability and fuzzy rules, conducting comparative analyses of the use of fuzzy rules and other explanation methods in handling three major types of multimodal data: sequence signals, medical images, and tabular data. Through this analysis, the review offers insights into optimal scenarios for applying fuzzy rules and provides recommendations for future research directions in this area.