Interpretable artificial intelligence (AI), also known as explainable AI, was indispensable in establishing trustable AI for bench-to-bedside translation, with substantial implications for human well-being. However, the majority of existing research in this area had centered on designing complex and sophisticated methods, regardless of their interpretability. Consequently, the main prerequisite for implementing trustworthy AI in medical domains had not been met. Scientists had developed various explanation methods for interpretable AI. Among these methods, fuzzy rules embedded in a fuzzy inference system (FIS) had emerged as a novel and powerful tool to bridge the communication gap between humans and advanced AI machines. However, there had been few reviews of the use of FISs in medical diagnosis. In addition, the application of fuzzy rules to different kinds of multimodal medical data had received insufficient attention, despite the potential use of fuzzy rules in designing appropriate methodologies for available datasets. This review provided a fundamental understanding of interpretability and fuzzy rules, conducted comparative analyses of the use of fuzzy rules and other explanation methods in handling three major types of multimodal data (i.e., sequence signals, medical images, and tabular data), and offered insights into appropriate fuzzy rule application scenarios and recommendations for future research.