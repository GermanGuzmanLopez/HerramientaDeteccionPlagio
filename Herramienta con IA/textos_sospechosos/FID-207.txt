{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs24 \cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Explainable artificial intelligence (AI), also referred to as interpretable AI, is essential for developing trustworthy AI systems for bench-to-bedside applications, significantly impacting human well-being. Despite this necessity, much of the existing research has focused on creating complex and sophisticated methods without considering their interpretability. As a result, the fundamental requirement for implementing reliable AI in medical fields has not been achieved. Various methods for explainable AI have been developed by scientists. Among these, fuzzy rules integrated within a fuzzy inference system (FIS) have emerged as an innovative and effective tool to bridge the communication gap between humans and advanced AI systems. However, there have been limited reviews on the use of FISs in medical diagnosis. Furthermore, the application of fuzzy rules to different types of multimodal medical data has not received adequate attention, despite their potential for designing suitable methodologies for the available datasets. This review aims to provide a foundational understanding of interpretability and fuzzy rules, conduct comparative analyses of fuzzy rules and other explanation methods for managing three major types of multimodal data (i.e., sequence signals, medical images, and tabular data), and offer insights into suitable scenarios for applying fuzzy rules along with recommendations for future research.}