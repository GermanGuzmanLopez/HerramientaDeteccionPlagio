{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs24 \cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Deep learning models combined with artificial intelligence, as one of the branches of machine learning, are extensively used in the field of computer vision technology, with significant advancements in image recognition, particularly in medical image analysis. One of its advantages is that it does not rely on human annotation; the computer can recognize and process feature information that humans may omit during the model training process, thereby achieving or even surpassing human-level accuracy. Due to the general lack of explainability caused by the opaque data processing in deep models, existing solutions primarily focus on establishing internal explainability, using attention mechanism interpretation for specific models, and employing tools like LIME to interpret otherwise opaque models. Methods to quantitatively assess interpretability are still being explored, particularly in evaluating medical decision-related models from the perspectives of both doctors and patients, with several proposed scales for reference. Current research on the application of artificial intelligence deep learning models in medical imaging tends to prioritize accuracy over explainability, resulting in a deficit in explainability and hindering the practical clinical implementation of these models. Consequently, analyzing the development of medical image analysis within artificial intelligence and computer vision technology, and finding a balance between accuracy and interpretability to develop deep learning models that both doctors and patients can trust, will become a key research focus in the industry moving forward.}